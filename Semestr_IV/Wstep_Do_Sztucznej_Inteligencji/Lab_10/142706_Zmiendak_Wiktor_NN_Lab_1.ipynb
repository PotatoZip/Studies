{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMMXLENOd7V-"
   },
   "source": [
    "# Wstęp do Sztucznej Inteligencji - rok akademicki 2022/2023\n",
    "\n",
    "Przed rozpoczęciem pracy z notatnikiem zmień jego nazwę zgodnie z wzorem: `NrAlbumu_Nazwisko_Imie_PoprzedniaNazwa`.\n",
    "\n",
    "Przed wysłaniem notatnika upewnij się, że rozwiązałeś wszystkie zadania/ćwiczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZZF68t-kiQ8"
   },
   "source": [
    "# Temat: Sztuczne Sieci Neuronowe - Lab 1 - Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwgxhLoJ1uGq"
   },
   "source": [
    "## Sztuczna komórka nerwowa\n",
    "Za pierwszy model sieci neuronowej uważa sie zainspirowany modelem biologicznym, model neuronu zaproponowany przez W. McCullacha i W. Pittsa w 1943 roku. Był to prosty neuron zdefiniowany jako układ z pewnym progiem wrażliwości posiadającym dwa typy wejść: pobudzające i hamujące. Założono, że układ ten może przyjmować tylko dwa stany: aktywny bądź nieaktywny.\n",
    "![neuron.png](http://torus.uck.pk.edu.pl/~amarsz/images/neuronl.png)\n",
    "\n",
    "Model neuronu McCullacha-Pittsa przedstawiony na powyższym rysunku, można opisać zależnością\n",
    "$$y = f\\left(\\sum\\limits_{i=1}^N{w_ix_i}+w_0\\right)$$\n",
    "gdzie:\n",
    "\n",
    "- funkcja $f(\\cdot)$ jest funkcją aktywacji neuronu,\n",
    "- wartości $x_i$ są sygnałami wejściowymi,\n",
    "- współczynniki $w_i$ są wagami połączeń synaptycznych,\n",
    "- współczynnik $w_0$ jest progiem wrażliwości neuronu (tzw. biasem).\n",
    "\n",
    "W oryginalnym modelu McCullocha-Pittsa jako funkcji aktywacji użyto funkcji skokowej, a dokładniej pseudofunkcji Heaviside’a\n",
    "$$ f(u) = \\left\\{\\begin{array}{l}\n",
    "1, \\quad u \\geq 0,\\\\\n",
    "0, \\quad u<0.\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTpCaibN1Ccp"
   },
   "source": [
    "## Perceptron\n",
    "Model neuronu McCullacha-Pittsa opisany powyżej z odpowiednio dobraną strategią uczenia nazywany jest _perceptronem Rosenblatta_ lub krótko _perceptronem_.\n",
    "\n",
    "Niech wektory sygnałów wejściowych oraz wartości wag będą odpowiednio postaci $x = [1, x_1, x_2, \\ldots, x_N]$ oraz\n",
    "$w = [w_0, w_1, \\ldots, w_N]$. Do wektora $x$ dołączono składową zerową $x_0 = 1$, stanowiącą sygnał polaryzacji, natomiast wartość wagi $w_0$ jest progiem wrażliwości neuronu (biasem). \n",
    "\n",
    "Wówczas model perceptronu możemy zapisać w postaci\n",
    "$$y = f(u) = f\\left(\\sum\\limits_{i=0}^N{w_ix_i}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rx6b2MiJ2tig"
   },
   "source": [
    "### Zastosowania prerceprtronu\n",
    "Perceptron na wyjściu zgodnie z funkcją aktywacji może przyjmować tylko dwie wartości 0 lub 1, wobec tego może on zostać wykorzystany do klasyfikacji wzorców pochodzących z dwóch różnych klas. Jeśli wartość sumatora jest dodatnia to wzorzec zostanie zaklasyfikowany do klasy 1, w przeciwnym przypadku będzie to klasa 0.\n",
    "\n",
    "Rozważmy problem w przestrzeni dwuwymiarowej ($N = 2$), wówczas perceptron odseparowuje klasy od siebie za pomocą linii prostej (w dowolnym wymiarze jest to ($N − 1$)-wymiarowa hiperpłaszczyzna) danej równaniem\n",
    "$$w_0x_0+w_1x_1+w_2x_2=0,$$\n",
    "co można zapisać w postaci równania kierunkowego prostej\n",
    "$$x_2=-\\frac{w_1}{w_2}x_1-\\frac{w_0}{w_2}.$$\n",
    "Na poniższym rysunku przedstawiona jest interpretacja geometryczna działania perceptronu w przypadku dwuwymiarowym, widzimy, że wagi sygnałów wejściowych wyznaczają nachylenie prostej, natomiast bias odpowiedzialny jest za przesunięcie prostej.\n",
    "![perc2d.png](http://torus.uck.pk.edu.pl/~amarsz/images/perc2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B7YUblnktrz"
   },
   "source": [
    "### Uczenie perceptronu\n",
    "Uczenie perceptronu należy do grupy uczenia z nauczycielem i polega na takim doborze wektora wag $w$, aby sygnały wyjściowe neuronu $y$ były najbliżej wartości pożądanej $d$. Najpopularniejszą metodą uczenia perceptronu jest tzw. _reguła perceptronu_, którą można opisać w postaci kilku kroków.\n",
    "\n",
    "Załóżmy, że dysponujemy zbiorem wektorów uczących postaci $\\{x^{(0)}, x^{(1)}, \\ldots, x^{(P)}\\}$ oraz odpowiadającym mu zbiorem wartości pożądanych $\\{d^{(0)}, d^{(1)},\\ldots, d^{(P)}\\}$.\n",
    "\n",
    "#### Reguła perceptronu\n",
    "1. Ustalamy $t = 0$.\n",
    "2. Ustalamy w sposób losowy początkowe wartości wektora wag $w$.\n",
    "3. Prezentujemy na wejścia perceptronu wektor uczący $x^{(t)}$.\n",
    "4. Obliczamy odpowiedź perceptronu $y$ zgodnie z wzorem $y = f\\left(\\sum\\limits_{i=0}^N{w_ix^{(t)}_i}\\right)$\n",
    "5. Porównujemy odpowiedź perceptronu $y$ z pożądaną odpowiedzią $d^{(t)}$.\n",
    "6. Modyfikujemy wartości wag według poniższych reguł, parametr $\\eta\\in(0,1)$ to _współczynnuk uczenia_:\n",
    "   - jeśli $y = d^{(t)}$ to wagi pozostają niezmienione,\n",
    "   - jeśli $y = 0$, a $d^{(t)}=1$ to $w_i = w_i + \\eta x_i^{(t)}$,\n",
    "   - jeśli $y = 1$, a $d^{(t)}=0$ to $w_i = w_i - \\eta x_i^{(t)}$.\n",
    "7. Jeżeli warunek zatrzymania nie jest spełniony, to ustalamy $t = t + 1$ i wracamy do kroku 3, w przeciwnym przypadku kończymy algorytm.\n",
    "\n",
    "Wykonanie powyższej procedury dla wszystkich wektorów uczących nazywamy _epoką uczenia_. W przypadku uczenia perceptronu wykonujemy tyle epok, aż wszystkie przykłady uczące będą dobrze sklasyfikowane lub błąd klasyfikacji będzie dostatecznie mały. Stabilność oraz szybkość uczenia tym algorytmem w istotny sposób zależy od doboru\n",
    "parametru $\\eta$. Współczynnik ten dobierany jest najczęściej w sposób empiryczny.\n",
    "\n",
    "Przykładowy przebieg uczenia dla danych `2D.csv`:\n",
    "![anim_perc.gif](http://torus.uck.pk.edu.pl/~amarsz/images/anim_perc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xPZCNLQ2zKO"
   },
   "source": [
    "## Zadanie 1\n",
    "Zaimplementuj model preceptronu w postaci klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "roJLXMvG4Dgx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    # Inicjalizator, ustawiający atrybut self.w jako wektor losowych wag, n ilość sygnałów wejściowych\n",
    "    def __init__(self, n):\n",
    "        self.w = np.random.rand(n)\n",
    "        \n",
    "    # Metoda obliczająca odpowiedz modelu dla zadanego sygnału wejściowego x=[1,x1,x2,...,xN]\n",
    "    def predict(self, x):\n",
    "        xBias = np.insert(x, 0, 1)\n",
    "        inputs = np.dot(self.w, xBias)\n",
    "        return 1 if inputs >= 0 else -1\n",
    "        \n",
    "    # Metoda uczenia według reguły perceptronu, xx - zbiór danych uczących, d - odpowiedzi,\n",
    "    # eta - współczynnik uczenia,\n",
    "    # tol - tolerancja (czyli jak duży błąd jesteśmy w stanie zaakceptować)\n",
    "    def train(self, xx, d, eta, tol):\n",
    "        error = tol + 1 \n",
    "        while error > tol:\n",
    "            error = 0\n",
    "            for i in range(len(xx)):\n",
    "                x = np.insert(xx[i], 0, 1)\n",
    "                y = self.predict(xx[i])\n",
    "                deltaW = eta * (d[i] - y) * x\n",
    "                self.w += deltaW\n",
    "                error += int(d[i] != y)\n",
    "        return self.w\n",
    "    # Metoda obliczająca błąd dla danych testowych xx\n",
    "    # zwraca błąd oraz wektor odpowiedzi perceptronu dla danych testowych\n",
    "    def evaluate_test(self, xx, d):\n",
    "        error = 0\n",
    "        predictions = []\n",
    "        for i in range(len(xx)):\n",
    "            y = self.predict(xx[i])\n",
    "            predictions.appned(y)\n",
    "            error += int(d[i] != y)\n",
    "        return error, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFD4AIrBffbv"
   },
   "source": [
    "## Zadanie 2\n",
    "- Stwórz dwa obiekty klasy `Perceptron`. \n",
    "- Wczytaj dane z plików `2D.csv` oraz `3D.csv.`\n",
    "- Pierwszy z perceptronów naucz klasyfikować dane z pliku `2D.csv`, ucz tylko na losowej części danych (np. 80%)\n",
    "- Drugi z perceptronów naucz klasyfikować dane z pliku `3D.csv`, ucz tylko na losowej części danych (np. 80%)\n",
    "- Oba zbiory danych są przykładami problemów liniowo separowalnych, a więc należy uczyć modele tak aby uzyskiwać dla danych uczących błąd równy zero.\n",
    "- Przedstaw rezultaty uczenia na wykresach, odpowiednio 2D lub 3D. Na wykresach powinny znaleźć się dane testowe, tzn. te które nie były wykorzystywane w trakcie uczenia oraz linia (płaszczyzna) rozdzielająca klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChKIJ_ZX2EDf"
   },
   "outputs": [],
   "source": [
    "# montowanie google drive\n",
    "import sys\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# upewniej się że poniższa ścieżka jest poprawna\n",
    "path_nb = r'/content/drive/My Drive/Colab Notebooks/WdSI_2023/T6_NN/'\n",
    "sys.path.append(path_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiUvCib9A6cc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### TWÓJ KOD TUTAJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C57dALyNj4Se"
   },
   "source": [
    "&copy; Katedra Informatyki, Politechnika Krakowska"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKlDlaHsKgF7Bn2hVrNrIU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
